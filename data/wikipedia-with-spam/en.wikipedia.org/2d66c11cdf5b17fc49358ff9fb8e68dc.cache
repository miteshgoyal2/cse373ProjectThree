https://en.wikipedia.org/wiki/Convolutional_neural_network
Convolutional neural network - Wikipedia
In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery. CNNs use...
convolutional neural network from wikipedia the free encyclopedia jump to navigation search for other uses see cnn disambiguation machine learning and data mining problems classification clustering regression anomaly detection association rules reinforcement learning structured prediction feature engineering feature learning online learning semisupervised learning unsupervised learning learning to rank grammar induction supervised learning classification • regression decision trees ensembles bagging boosting random forest knn linear regression naive bayes neural networks logistic regression perceptron relevance vector machine rvm support vector machine svm clustering birch hierarchical kmeans expectation–maximization em dbscan optics meanshift dimensionality reduction factor analysis cca ica lda nmf pca tsne structured prediction graphical models bayes net crf hmm anomaly detection knn local outlier factor neural nets autoencoder deep learning multilayer perceptron rnn restricted boltzmann machine som convolutional neural network reinforcement learning qlearning sarsa temporal difference td theory biasvariance dilemma computational learning theory empirical risk minimization occam learning pac learning statistical learning vc theory machinelearning venues nips icml ml jmlr arxivcslg related articles list of datasets for machinelearning research outline of machine learning machine learning portal v t e in machine learning a convolutional neural network cnn or convnet is a class of deep feedforward artificial neural networks that has successfully been applied to analyzing visual imagery cnns use a variation of multilayer perceptrons designed to require minimal preprocessing1 they are also known as shift invariant or space invariant artificial neural networks siann based on their sharedweights architecture and translation invariance characteristics23 convolutional networks were inspired by biological processes4 in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field the receptive fields of different neurons partially overlap such that they cover the entire visual field cnns use relatively little preprocessing compared to other image classification algorithms this means that the network learns the filters that in traditional algorithms were handengineered this independence from prior knowledge and human effort in feature design is a major advantage they have applications in image and video recognition recommender systems5 and natural language processing6 contents 1 design 11 convolutional 12 pooling 13 fully connected 14 weights 2 time delay neural networks 3 history 31 receptive fields 32 neocognitron 321 lenet5 33 shiftinvariant neural network 34 neural abstraction pyramid 35 gpu implementations 4 distinguishing features 5 building blocks 51 convolutional layer 511 local connectivity 512 spatial arrangement 513 parameter sharing 52 pooling layer 53 relu layer 54 fully connected layer 55 loss layer 6 choosing hyperparameters 61 number of filters 62 filter shape 63 max pooling shape 7 regularization methods 71 empirical 711 dropout 712 dropconnect 713 stochastic pooling 714 artificial data 72 explicit 721 early stopping 722 number of parameters 723 weight decay 724 max norm constraints 8 hierarchical coordinate frames 9 applications 91 image recognition 92 video analysis 93 natural language processing 94 drug discovery 95 checkers 96 go 10 finetuning 11 extensions 111 deep qnetworks 112 deep belief networks 12 common libraries 13 common apis 14 popular culture 15 see also 16 references 17 external links designedit a cnn consists of an input and an output layer as well as multiple hidden layers the hidden layers are either convolutional pooling or fully connected convolutionaledit convolutional layers apply a convolution operation to the input passing the result to the next layer the convolution emulates the response of an individual neuron to visual stimuli7 each convolutional neuron processes data only for its receptive fieldclarification needed tiling allows cnns to tolerate translation of the input image eg translation rotation perspective distortionclarification needed although fully connected feedforward neural networks can be used to learn features as well as classify data it is not practical to apply this architecture to images a very high number of neurons would be necessary even in a shallow opposite of deep architecturecitation needed due to the very large input sizes associated with images where each pixel is a relevant data point the convolution operation brings a solution to this problem as it reduces the number of free parameters allowing the network to be deeper with fewer parameters8 in other words it resolves the vanishing or exploding gradients problem in training traditional multilayer neural networks with many layers by using backpropagationcitation needed poolingedit convolutional networks may include local or global pooling layersclarification needed which combine the outputs of neuron clusters at one layer into a single neuron in the next layer910 for example max pooling uses the maximum value from each of a cluster of neurons at the prior layer11 another example is average pooling which uses the average value from each of a cluster of neurons at the prior layercitation needed fully connectededit fully connected layers connect every neuron in one layer to every neuron in another layer it is in principle the same as the traditional multilayer perceptron neural network mlp weightsedit cnns share weights in convolutional layers which means that the same filter weights bankclarification needed is used for each receptive fieldclarification needed in the layer this reduces memory footprint and improves performancehow1 time delay neural networksedit some time delay neural networks use a similar architecture especially those for image recognition or classification tasks since the tiling of neuron outputs can be done in timed stages in a manner useful for analysis of images12 historyedit cnn design follows vision processing in living organismscitation needed receptive fieldsedit work by hubel and wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field provided the eyes are not moving the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive fieldcitation needed neighboring cells have similar and overlapping receptive fieldscitation needed receptive field size and location varies systematically across the cortex to form a complete map of visual spacecitation needed the cortex in each hemisphere represents the contralateral visual fieldcitation needed their 1968 paper13 identified two basic visual cell types in the brain simple cells whose output is maximized by straight edges having particular orientations within their receptive field complex cells which have larger receptive fields whose output is insensitive to the exact position of the edges in the field neocognitronedit the neocognitron 14 was introduced in 19801115 the neocognitron does not require units located at multiple network positions to have the same trainable weights this idea appears in 1986 in the book version of the original backpropagation paper16 figure 14 neocognitrons were developed in 1988 for temporal signalsclarification needed17 their design was improved in 199818 generalized in 200319 and simplified in the same year20 lenet5edit lenet5 a pioneering 7level convolutional network by lecun et al18 that classifies digits was applied by several banks to recognise handwritten numbers on checks cheques digitized in 32x32 pixel images the ability to process higher resolution images requires larger and more convolutional layers so this technique is constrained by the availability of computing resources shiftinvariant neural networkedit similarly a shift invariant neural network was proposed for image character recognition in 198823 the architecture and training algorithm were modified in 199121 and applied for medical image processing22 and automatic detection of breast cancer in mammograms23 a different convolutionbased design was proposed in 198824 for application to decomposition of onedimensional electromyography convolved signals via deconvolution this design was modified in 1989 to other deconvolutionbased designs2526 neural abstraction pyramidedit the feedforward architecture of convolutional neural networks was extended in the neural abstraction pyramid27 by lateral and feedback connections the resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities in contrast to previous models imagelike outputs at the highest resolution were generated gpu implementationsedit following the 2005 paper that established the value of gpgpu for machine learning28 several publications described more efficient ways to train convolutional neural networks using gpus29303132 in 2011 they were refined and implemented on a gpu with impressive results9 in 2012 ciresan et al significantly improved on the best performance in the literature for multiple image databases including the mnist database the norb database the hwdb10 dataset chinese characters the cifar10 dataset dataset of 60000 32x32 labeled rgb images11 and the imagenet dataset33 distinguishing featuresedit while traditional multilayer perceptron mlp models were successfully used for image recognitionexamples needed due to the full connectivity between nodes they suffer from the curse of dimensionality and thus do not scale well to higher resolution images cnn layers arranged in 3 dimensions for example in cifar10 images are only of size 32x32x3 32 wide 32 high 3 color channels so a single fully connected neuron in a first hidden layer of a regular neural network would have 32323 3072 weights a 200x200 image however would lead to neurons that have 2002003 120000 weights also such network architecture does not take into account the spatial structure of data treating input pixels which are far apart the same as pixels that are close togethercitation needed thus full connectivity of neurons is wasteful for the purpose of image recognitionclarification needed convolutional neural networks are biologically inspired variants of multilayer perceptrons designed to emulate the behaviour of a visual cortexcitation needed these models mitigate the challenges posed by the mlp architecture by exploiting the strong spatially local correlation present in natural images as opposed to mlps cnns have the following distinguishing features 3d volumes of neurons the layers of a cnn have neurons arranged in 3 dimensions width height and depth the neurons inside a layer are connected to only a small region of the layer before it called a receptive field distinct types of layers both locally and completely connected are stacked to form a cnn architecture local connectivity following the concept of receptive fields cnns exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers the architecture thus ensures that the learnt filters produce the strongest response to a spatially local input pattern stacking many such layers leads to nonlinear filters that become increasingly global ie responsive to a larger region of pixel space this allows the network to first create representations of small parts of the input then from them assemble representations of larger areas shared weights in cnns each filter is replicated across the entire visual field these replicated units share the same parameterization weight vector and bias and form a feature map this means that all the neurons in a given convolutional layer respond to the same feature within their specific response field replicating units in this way allows for features to be detected regardless of their position in the visual field thus constituting the property of translation invariance together these properties allow cnns to achieve better generalization on vision problems weight sharing dramatically reduces the number of free parameters learned thus lowering the memory requirements for running the network decreasing the memory footprint allows the training of larger more powerful networks building blocksedit this section needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed june 2017 learn how and when to remove this template message a cnn architecture is formed by a stack of distinct layers that transform the input volume into an output volume eg holding the class scores through a differentiable function a few distinct types of layers are commonly used we discuss them further below neurons of a convolutional layer blue connected to their receptive field red convolutional layeredit the convolutional layer is the core building block of a cnn the layers parameters consist of a set of learnable filters or kernels which have a small receptive field but extend through the full depth of the input volume during the forward pass each filter is convolved across the width and height of the input volume computing the dot product between the entries of the filter and the input and producing a 2dimensional activation map of that filter as a result the network learns filters that activate when it detects some specific type of feature at some spatial position in the input stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map local connectivityedit when dealing with highdimensional inputs such as images it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers each neuron is connected to only a small region of the input volume the extent of this connectivity is a hyperparameter called the receptive field of the neuron the connections are local in space along width and height but always extend along the entire depth of the input volume such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern spatial arrangementedit three hyperparameters control the size of the output volume of the convolutional layer the depth stride and zeropadding the depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume these neurons learn to activate for different features in the input for example if the first convolutional layer takes the raw image as input then different neurons along the depth dimension may activate in the presence of various oriented edges or blobs of color stride controls how depth columns around the spatial dimensions width and height are allocated when the stride is 1 then we move the filters one pixel at a time this leads to heavily overlapping receptive fields between the columns and also to large output volumes when the stride is 2 or rarely 3 or more then the filters jump 2 pixels at a time as they slide around the receptive fields overlap less and the resulting output volume has smaller spatial dimensions34 sometimes it is convenient to pad the input with zeros on the border of the input volume the size of this padding is a third hyperparameter padding provides control of the output volume spatial size in particular sometimes it is desirable to exactly preserve the spatial size of the input volume the spatial size of the output volume can be computed as a function of the input volume size w displaystyle w the kernel field size of the conv layer neurons k displaystyle k the stride with which they are applied s displaystyle s and the amount of zero padding p displaystyle p used on the border the formula for calculating how many neurons fit in a given volume is given by w − k 2 p s 1 displaystyle wk2ps1 if this number is not an integer then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way in general setting zero padding to be p k − 1 2 displaystyle pk12 when the stride is s 1 displaystyle s1 ensures that the input volume and output volume will have the same size spatially though its generally not completely necessary to use up all of the neurons of the previous layer for example you may decide to use just a portion of padding parameter sharingedit a parameter sharing scheme is used in convolutional layers to control the number of free parameters it relies on one reasonable assumption that if a patch feature is useful to compute at some spatial position then it should also be useful to compute at other positions in other words denoting a single 2dimensional slice of depth as a depth slice we constrain the neurons in each depth slice to use the same weights and bias since all neurons in a single depth slice share the same parameters then the forward pass in each depth slice of the conv layer can be computed as a convolution of the neurons weights with the input volume hence the name convolutional layer therefore it is common to refer to the sets of weights as a filter or a kernel which is convolved with the input the result of this convolution is an activation map and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume parameter sharing contributes to the translation invariance of the cnn architecture sometimes the parameter sharing assumption may not make sense this is especially the case when the input images to a cnn have some specific centered structure in which we expect completely different features to be learned on different spatial locations one practical example is when the input are faces that have been centered in the image we might expect different eyespecific or hairspecific features to be learned in different parts of the image in that case it is common to relax the parameter sharing scheme and instead simply call the layer a locally connected layer pooling layeredit max pooling with a 2x2 filter and stride 2 another important concept of cnns is pooling which is a form of nonlinear downsampling there are several nonlinear functions to implement pooling among which max pooling is the most common it partitions the input image into a set of nonoverlapping rectangles and for each such subregion outputs the maximum the intuition is that the exact location of a feature is less important than its rough location relative to other features the pooling layer serves to progressively reduce the spatial size of the representation to reduce the number of parameters and amount of computation in the network and hence to also control overfitting it is common to periodically insert a pooling layer between successive convolutional layers in a cnn architecture the pooling operation provides another form of translation invariance the pooling layer operates independently on every depth slice of the input and resizes it spatially the most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height discarding 75 of the activations in this case every max operation is over 4 numbers the depth dimension remains unchanged in addition to max pooling the pooling units can use other functions such as average pooling or l2norm pooling average pooling was often used historically but has recently fallen out of favor compared to max pooling which works better in practice35 due to the aggressive reduction in the size of the representation the trend is towards using smaller filters36 or discarding the pooling layer altogether37 roi pooling to size 2x2 in this example region proposal an input parameter has size 7x5 region of interest pooling also known as roi pooling is a variant of max pooling in which output size is fixed and input rectangle is a parameter38 pooling is an important component of convolutional neural networks for object detection based on fast rcnn39 architecture relu layeredit relu is the abbreviation of rectified linear units this layer applies the nonsaturating activation function f x max 0 x displaystyle fxmax0x it increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer other functions are also used to increase nonlinearity for example the saturating hyperbolic tangent f x tanh ⁡ x displaystyle fxtanhx f x tanh ⁡ x displaystyle fxtanhx and the sigmoid function f x 1 e − x − 1 displaystyle fx1ex1 relu is preferable to other functions because it trains the neural network several times faster40 without a significant penalty to generalisation accuracy fully connected layeredit finally after several convolutional and max pooling layers the highlevel reasoning in the neural network is done via fully connected layers neurons in a fully connected layer have connections to all activations in the previous layer as seen in regular neural networks their activations can hence be computed with a matrix multiplication followed by a bias offset loss layeredit the loss layer specifies how training penalizes the deviation between the predicted and true labels and is normally the final layer various loss functions appropriate for different tasks may be used there softmax loss is used for predicting a single class of k mutually exclusive classes sigmoid crossentropy loss is used for predicting k independent probability values in 0 1 displaystyle 01 euclidean loss is used for regressing to realvalued labels − ∞ ∞ displaystyle infty infty typical cnn architecture choosing hyperparametersedit this section needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed june 2017 learn how and when to remove this template message cnns use more hyperparameters than a standard mlp while the usual rules for learning rates and regularization constants still apply the following should be kept in mind when optimising number of filtersedit since feature map size decreases with depth layers near the input layer will tend to have fewer filters while higher layers can have more to equalize computation at each layer the feature x pixel position product is kept roughly constant across layers preserving more information about the input would require keeping the total number of activations number of feature maps times number of pixel positions nondecreasing from one layer to the next the number of feature maps directly controls capacity and depends on the number of available examples and task complexity filter shapeedit common field shapes found in the literature vary greatly and are usually chosen based on the dataset the challenge is thus to find the right level of granularity so as to create abstractions at the proper scale given a particular dataset max pooling shapeedit typical values are 2x2 very large input volumes may warrant 4x4 pooling in the lowerlayers however choosing larger shapes will dramatically reduce the dimension of the signal and may result in excess information loss often nonoverlapping pooling windows perform best35 regularization methodsedit main article regularization this section needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed june 2017 learn how and when to remove this template message empiricaledit dropoutedit because a fully connected layer occupies most of the parameters it is prone to overfitting one method to reduce overfitting is dropout4142 at each training stage individual nodes are either dropped out of the net with probability 1 − p displaystyle 1p or kept with probability p displaystyle p so that a reduced network is left incoming and outgoing edges to a droppedout node are also removed only the reduced network is trained on the data in that stage the removed nodes are then reinserted into the network with their original weights in the training stages the probability that a hidden node will be dropped is usually 05 for input nodes this should be much lower intuitively because information is directly lost when input nodes are ignored at testing time after training has finished we would ideally like to find a sample average of all possible 2 n displaystyle 2n droppedout networks unfortunately this is unfeasible for large values of n displaystyle n however we can find an approximation by using the full network with each nodes output weighted by a factor of p displaystyle p so the expected value of the output of any node is the same as in the training stages this is the biggest contribution of the dropout method although it effectively generates 2 n displaystyle 2n neural nets and as such allows for model combination at test time only a single network needs to be tested by avoiding training all nodes on all training data dropout decreases overfitting in neural nets the method also significantly improves the speed of training this makes model combination practical even for deep neural nets the technique seems to reduce node interactions leading them to learn more robust features that better generalize to new data dropconnectedit dropconnect43 is the generalization of dropout in which each connection rather than each output unit can be dropped with probability 1 − p displaystyle 1p each unit thus receives input from a random subset of units in the previous layer dropconnect is similar to dropout as it introduces dynamic sparsity within the model but differs in that the sparsity is on the weights rather than the output vectors of a layer in other words the fully connected layer with dropconnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage stochastic poolingedit a major drawback to dropout is that it does not have the same benefits for convolutional layers where the neurons are not fully connected in stochastic pooling44 the conventional deterministic pooling operations are replaced with a stochastic procedure where the activation within each pooling region is picked randomly according to a multinomial distribution given by the activities within the pooling region the approach is hyperparameter free and can be combined with other regularization approaches such as dropout and data augmentation an alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image each having small local deformations this is similar to explicit elastic deformations of the input images45 which delivers excellent mnist performance using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below artificial dataedit since the degree of model overfitting is determined by both its power and the amount of training it receives providing a convolutional network with more training examples can reduce overfitting since these networks are usually trained with all available data one approach is to either generate new data from scratch if possible or perturb existing data to create new ones for example input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original46 explicitedit early stoppingedit main article early stopping one of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur it comes with the disadvantage that the learning process is halted number of parametersedit another simple way to prevent overfitting is to limit the number of parameters typically by limiting the number of hidden units in each layer or limiting network depth for convolutional networks the filter size also affects the number of parameters limiting the number of parameters restricts the predictive power of the network directly reducing the complexity of the function that it can perform on the data and thus limits the amount of overfitting this is equivalent to a zero norm weight decayedit a simple form of added regularizer is weight decay which simply adds an additional error proportional to the sum of weights l1 norm or squared magnitude l2 norm of the weight vector to the error at each node the level of acceptable model complexity can be reduced by increasing the proportionality constant thus increasing the penalty for large weight vectors l2 regularization is the most common form of regularization it can be implemented by penalizing the squared magnitude of all parameters directly in the objective the l2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot l1 regularization is another common form it is possible to combine l1 with l2 regularization this is called elastic net regularization the l1 regularization leads the weight vectors to become sparse during optimization in other words neurons with l1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs max norm constraintsedit another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint in practice this corresponds to performing the parameter update as normal and then enforcing the constraint by clamping the weight vector w → displaystyle vec w of every neuron to satisfy ∥ w → ∥ 2 c displaystyle vec w2c typical values of c displaystyle c are order of 3–4 some papers report improvements47 when using this form of regularization hierarchical coordinate framesedit pooling loses the precise spatial relationships between highlevel parts such as nose and mouth in a face image these relationships are needed for identity recognition overlapping the pools so that each feature occurs in multiple pools helps retain the information translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint such as a different orientation or scale on the other hand people are very good at extrapolating after seeing a new shape once they can recognize it from a different viewpoint48 currently the common way to deal with this problem is to train the network on transformed data in different orientations scales lighting etc so that the network can cope with these variations this is computationally intensive for large datasets the alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina the pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features coordinate frame49 thus one way of representing something is to embed the coordinate frame within it once this is done large features can be recognized by using the consistency of the poses of their parts eg nose and mouth poses make a consistent prediction of the pose of the whole face using this approach ensures that the higher level entity eg face is present when the lower level eg nose and mouth agree on its prediction of the pose the vectors of neuronal activity that represent pose pose vectors allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints this is similar to the way the human visual system imposes coordinate frames in order to represent shapes50 applicationsedit image recognitionedit cnns are often used in image recognition systems in 2012 an error rate of 023 percent on the mnist database was reported11 another paper on using cnn for image classification reported that the learning process was surprisingly fast in the same paper the best published results as of 2011 were achieved in the mnist database and the norb database9 when applied to facial recognition cnns achieved a large decrease in error rate51 another paper reported a 976 percent recognition rate on 5600 still images of more than 10 subjects4 cnns were used to assess video quality in an objective way after manual training the resulting system had a very low root mean square error12 the imagenet large scale visual recognition challenge is a benchmark in object classification and detection with millions of images and hundreds of object classes in the ilsvrc 201452 a largescale visual recognition challenge almost every highly ranked team used cnn as their basic framework the winner googlenet53 the foundation of deepdream increased the mean average precision of object detection to 0439329 and reduced classification error to 006656 the best result to date its network applied more than 30 layers as of that performance of convolutional neural networks on the imagenet tests was close to that of humans54 the best algorithms still struggle with objects that are small or thin such as a small ant on a stem of a flower or a person holding a quill in their hand they also have trouble with images that have been distorted with filters an increasingly common phenomenon with modern digital cameras by contrast those kinds of images rarely trouble humans humans however tend to have trouble with other issues for example they are not good at classifying objects into finegrained categories such as the particular breed of dog or species of bird whereas convolutional neural networks handle this in 2015 a manylayered cnn demonstrated the ability to spot faces from a wide range of angles including upside down even when partially occluded with competitive performance the network trained on a database of 200000 images that included faces at various angles and orientations and a further 20 million images without faces they used batches of 128 images over 50000 iterations55 video analysisedit compared to image data domains there is relatively little work on applying cnns to video classification video is more complex than images since it has another temporal dimension however some extensions of cnns into the video domain have been explored one approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space5657 another way is to fuse the features of two convolutional neural networks one for the spatial and one for the temporal stream5859 unsupervised learning schemes for training spatiotemporal features have been introduced based on convolutional gated restricted boltzmann machines60 and independent subspace analysis61 natural language processingedit cnns have also explored natural language processing cnn models are effective for various nlp problems and achieved excellent results in semantic parsing62 search query retrieval63 sentence modeling64 classification65 prediction66 and other traditional nlp tasks67 drug discoveryedit cnns have been used in drug discovery predicting the interaction between molecules and biological proteins can identify potential treatments in 2015 atomwise introduced atomnet the first deep learning neural network for structurebased rational drug design68 the system trains directly on 3dimensional representations of chemical interactions similar to how image recognition networks learn to compose smaller spatially proximate features into larger complex structures69 atomnet discovers chemical features such as aromaticity sp3 carbons and hydrogen bonding subsequently atomnet was used to predict novel candidate biomolecules for multiple disease targets most notably treatments for the ebola virus70 and multiple sclerosis71 checkersedit cnns have been used in the game of checkers from 1999–2001 fogel and chellapilla published papers showing how a convolutional neural network could learn to play checkers using coevolution the learning process did not use prior human professional games but rather focused on a minimal set of information contained in the checkerboard the location and type of pieces and the piece differential ultimately the program blondie24 was tested on 165 games against players and ranked in the highest 047273 it also earned a wins against the program chinook at its expert level of play74 goedit cnns have been used in computer go in december 2014 clark and storkey published a paper showing that a cnn trained by supervised learning from a database of human professional games could outperform gnu go and win some games against monte carlo tree search fuego 11 in a fraction of the time it took fuego to play75 later it was announced that a large 12layer convolutional neural network had correctly predicted the professional move in 55 of positions equalling the accuracy of a 6 dan human player when the trained convolutional network was used directly to play games of go without any search it beat the traditional search program gnu go in 97 of games and matched the performance of the monte carlo tree search program fuego simulating ten thousand playouts about a million positions per move76 a couple of cnns for choosing moves to try policy network and evaluating positions value network driving mcts were used by alphago the first to beat the best human player at the time77 finetuningedit for many applications little training data is available convolutional neural networks usually require a large amount of training data in order to avoid overfitting a common technique is to train the network on a larger data set from a related domain once the network parameters have converged an additional training step is performed using the indomain data to finetune the network weights this allows convolutional networks to be successfully applied to problems with small training sets78 extensionsedit deep qnetworksedit a deep qnetwork dqn is a type of deep learning model that combines a deep cnn with qlearning a form of reinforcement learning unlike earlier reinforcement learning agents dqns can learn directly from highdimensional sensory inputs preliminary results were presented in 2014 with an accompanying paper in february 201579 the research described an application to atari 2600 gaming other deep reinforcement learning models preceded it80 deep belief networksedit main article deep belief network convolutional deep belief networks cdbn have structure very similar to convolutional neural networks and are trained similarly to deep belief networks therefore they exploit the 2d structure of images like cnns do and make use of pretraining like deep belief networks they provide a generic structure that can be used in many image and signal processing tasks benchmark results on standard image datasets like cifar81 have been obtained using cdbns82 common librariesedit caffe a popular library for convolutional neural networks created by the berkeley vision and learning center bvlc it supports both cpu and gpu developed in c and has python and matlab wrappers deeplearning4j deep learning in java and scala on multigpuenabled spark a generalpurpose deep learning library for the jvm production stack running on a c scientific computing engine allows the creation of custom layers integrates with hadoop and kafka deeplearninghs deep learning in haskell supports computations with cuda matconvnet a convnet implementation in matlab mxnet an opensource deep learning framework which is scalable including support for multiple gpus and cpus in distribution it supports interfaces in multiple languages c python julia matlab javascript go r scala perl wolfram language neon the fastest framework for convolutional neural networks and deep learning with support for gpu and cpu backends the frontend is in python while the fast kernels are written in custom shader assembly created by nervana systems which was acquired by intel tensorflow apache 20licensed theanolike library with support for cpu gpu and googles proprietary tpu83 mobile theano the reference deeplearning library for python with an api largely compatible with the popular numpy library allows user to write symbolic mathematical expressions then automatically generates their derivatives saving the user from having to code gradients or backpropagation these symbolic expressions are automatically compiled to cuda code for a fast onthegpu implementation torch wwwtorchch a scientific computing framework with wide support for machine learning algorithms written in c and lua the main author is ronan collobert and it is now used at facebook ai research and twitter microsoft cognitive toolkit a deep learning toolkit written by microsoft with several unique features enhancing scalability over multiple nodes it supports fullfledged interfaces for training in c and python and with additional support for model inference in c and java common apisedit keras a high level api written in python for tensorflow and theano convolutional neural networks84 popular cultureedit convolutional neural networks are mentioned in the 2017 novel infinity born85 see alsoedit convolution deep learning neocognitron scaleinvariant feature transform time delay neural network vision processing unit referencesedit a b lecun yann lenet5 convolutional neural networks retrieved 16 november 2013  a b zhang wei 1988 shiftinvariant pattern recognition neural network and its optical architecture proceedings of annual conference of the japan society of applied physics  a b zhang wei 1990 parallel distributed processing model with local spaceinvariant interconnections and its optical architecture applied optics 29 32 4790–7 bibcode1990apopt294790z pmid 20577468 doi101364ao29004790  a b matusugu masakazu katsuhiko mori yusuke mitari yuji kaneda 2003 subject independent facial expression recognition with robust face detection using a convolutional neural network pdf neural networks 16 5 555–559 doi101016s0893608003001151 retrieved 17 november 2013  van den oord aaron dieleman sander schrauwen benjamin 20130101 burges c j c bottou l welling m ghahramani z weinberger k q eds deep contentbased music recommendation pdf curran associates inc pp 2643–2651  collobert ronan weston jason 20080101 a unified architecture for natural language processing deep neural networks with multitask learning proceedings of the 25th international conference on machine learning icml 08 new york ny usa acm 160–167 isbn 9781605582054 doi10114513901561390177  convolutional neural networks lenet – deeplearning 01 documentation deeplearning 01 lisa lab retrieved 31 august 2013  habibi aghdam hamed guide to convolutional neural networks  a practical application to trafficsign detection and classification heravi elnaz jahani cham switzerland isbn 9783319575490 oclc 987790957  a b c ciresan dan ueli meier jonathan masci luca m gambardella jurgen schmidhuber 2011 flexible high performance convolutional neural networks for image classiﬁcation pdf proceedings of the twentysecond international joint conference on artificial intelligencevolume volume two 2 1237–1242 retrieved 17 november 2013  krizhevsky alex imagenet classification with deep convolutional neural networks pdf retrieved 17 november 2013  a b c d ciresan dan meier ueli schmidhuber jürgen june 2012 multicolumn deep neural networks for image classification 2012 ieee conference on computer vision and pattern recognition new york ny institute of electrical and electronics engineers ieee 3642–3649 isbn 9781467312264 oclc 812295155 arxiv12022745v1  doi101109cvpr20126248110 retrieved 20131209  a b le callet patrick christian viardgaudin dominique barba 2006 a convolutional neural network approach for objective video quality assessment pdf ieee transactions on neural networks 17 5 1316–1327 pmid 17001990 doi101109tnn2006879766 retrieved 17 november 2013  hubel d h wiesel t n 19680301 receptive fields and functional architecture of monkey striate cortex the journal of physiology 195 1 215–243 issn 00223751 pmc 1557912  pmid 4966457 doi101113jphysiol1968sp008455  lecun yann bengio yoshua hinton geoffrey 2015 deep learning nature 521 7553 436–444 bibcode2015natur521436l pmid 26017442 doi101038nature14539  fukushima kunihiko 1980 neocognitron a selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position pdf biological cybernetics 36 4 193–202 pmid 7370364 doi101007bf00344251 retrieved 16 november 2013  david e rumelhart geoffrey e hinton ronald j wiliams 1986 chapter 8  learning internal representations by errorpropagation in rumelhart david e mcclelland jamesl parallel distributed processing volume 1 pdf mit press pp 319–362 isbn 9780262680530  homma toshiteru les atlas robert marks ii 1988 an artificial neural network for spatiotemporal bipolar patters application to phoneme classification pdf advances in neural information processing systems 1 31–40  a b lecun yann léon bottou yoshua bengio patrick haffner 1998 gradientbased learning applied to document recognition pdf proceedings of the ieee 86 11 2278–2324 doi1011095726791 retrieved october 7 2016  s behnke hierarchical neural networks for image interpretation volume 2766 of lecture notes in computer science springer 2003 simard patrice david steinkraus and john c platt best practices for convolutional neural networks applied to visual document analysis in icdar vol 3 pp 958–962 2003 zhang wei 1991 error back propagation with minimumentropy weights a technique for better generalization of 2d shiftinvariant nns proceedings of the international joint conference on neural networks  zhang wei 1991 image processing of human corneal endothelium based on a learning network applied optics 30 29 4211–7 bibcode1991apopt304211z pmid 20706526 doi101364ao30004211  zhang wei 1994 computerized detection of clustered microcalcifications in digital mammograms using a shiftinvariant artificial neural network medical physics 21 4 517–24 bibcode1994medph21517z pmid 8058017 doi1011181597177  daniel graupe ruey wen liu george s moschytzapplications of neural networks to medical signal processing in proc 27th ieee decision and control conf pp 343–347 1988 daniel graupe boris vern g gruener aaron field and qiu huang decomposition of surface emg signals into single fiber action potentials by means of neural network proc ieee international symp on circuits and systems pp 1008–1011 1989 qiu huang daniel graupe yi fang huang ruey wen liuidentification of firing patterns of neuronal signals in proc 28th ieee decision and control conf pp 266–271 1989 behnke sven 2003 hierarchical neural networks for image interpretation pdf lecture notes in computer science 2766 springer isbn 9783540407225 doi101007b11963  dave steinkraus patrice simard ian buck 2005 using gpus for machine learning algorithms 12th international conference on document analysis and recognition icdar 2005 pp 1115–1119  kumar chellapilla sid puri patrice simard 2006 high performance convolutional neural networks for document processing in lorette guy tenth international workshop on frontiers in handwriting recognition suvisoft  hinton ge osindero s teh yw jul 2006 a fast learning algorithm for deep belief nets neural computation 18 7 1527–54 pmid 16764513 doi101162neco20061871527  bengio yoshua lamblin pascal popovici dan larochelle hugo 2007 greedy layerwise training of deep networks advances in neural information processing systems 153–160  ranzato marcaurelio poultney christopher chopra sumit lecun yann 2007 efficient learning of sparse representations with an energybased model pdf advances in neural information processing systems  10 deng jia et al imagenet a largescale hierarchical image databasecomputer vision and pattern recognition 2009 cvpr 2009 ieee conference on ieee 2009 cs231n convolutional neural networks for visual recognition cs231ngithubio retrieved 20170425  a b scherer dominik müller andreas c behnke sven 2010 evaluation of pooling operations in convolutional architectures for object recognition pdf artificial neural networks icann 20th international conference on thessaloniki greece springer pp 92–101  graham benjamin 20141218 fractional maxpooling arxiv14126071  cscv  springenberg jost tobias dosovitskiy alexey brox thomas riedmiller martin 20141221 striving for simplicity the all convolutional net arxiv14126806  cslg  grel tomasz 20170228 region of interest pooling explained deepsenseio  girshick ross 20170927 fast rcnn arxiv150408083  cscv  krizhevsky a sutskever i hinton g e 2012 imagenet classification with deep convolutional neural networks pdf advances in neural information processing systems 1 1097–1105  srivastava nitish c geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov 2014 dropout a simple way to prevent neural networks from overfitting pdf journal of machine learning research 15 1 1929–1958  carlos e perez a pattern language for deep learning  regularization of neural networks using dropconnect icml 2013 jmlr wcp jmlrorg retrieved 20151217  zeiler matthew d fergus rob 20130115 stochastic pooling for regularization of deep convolutional neural networks arxiv13013557  cslg  best practices for convolutional neural networks applied to visual document analysis – microsoft research researchmicrosoftcom retrieved 20151217  hinton geoffrey e srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan r 2012 improving neural networks by preventing coadaptation of feature detectors arxiv12070580  csne  dropout a simple way to prevent neural networks from overfitting jmlrorg retrieved 20151217  hinton geoffrey 1979 some demonstrations of the effects of structural descriptions in mental imagery cognitive science 3 3 231–250 doi101016s0364021379800087  rock irvin the frame of reference the legacy of solomon asch essays in cognition and social psychology 1990 243–268 j hinton coursera lectures on neural networks 2012 url httpswwwcourseraorglearnneuralnetworks lawrence steve c lee giles ah chung tsoi andrew d back 1997 face recognition a convolutional neural network approach neural networks ieee transactions on 8 1 98–113 citeseerx 1011925813  doi10110972554195  imagenet large scale visual recognition competition 2014 ilsvrc2014 retrieved 30 january 2016  szegedy christian liu wei jia yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew 2014 going deeper with convolutions computing research repository arxiv14094842   russakovsky olga deng jia su hao krause jonathan satheesh sanjeev ma sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander c feifei li 2014 image net large scale visual recognition challenge arxiv14090575  cscv  the face detection algorithm set to revolutionize image search technology review february 16 2015 retrieved 27 october 2017  baccouche moez mamalet franck wolf christian garcia christophe baskurt atilla 20111116 sequential deep learning for human action recognition in salah albert ali lepri bruno human behavior unterstanding lecture notes in computer science 7065 springer berlin heidelberg pp 29–39 isbn 9783642254451 doi10100797836422544684  ji shuiwang xu wei yang ming yu kai 20130101 3d convolutional neural networks for human action recognition ieee transactions on pattern analysis and machine intelligence 35 1 221–231 issn 01628828 pmid 22392705 doi101109tpami201259  karpathy andrej et al largescale video classification with convolutional neural networks ieee conference on computer vision and pattern recognition cvpr 2014 simonyan karen zisserman andrew 2014 twostream convolutional networks for action recognition in videos arxiv14062199  cscv  2014 taylor graham w fergus rob lecun yann bregler christoph 20100101 convolutional learning of spatiotemporal features proceedings of the 11th european conference on computer vision part vi eccv10 berlin heidelberg springerverlag 140–153 isbn 3642155669  le q v zou w y yeung s y ng a y 20110101 learning hierarchical invariant spatiotemporal features for action recognition with independent subspace analysis proceedings of the 2011 ieee conference on computer vision and pattern recognition cvpr 11 washington dc usa ieee computer society 3361–3368 isbn 9781457703942 doi101109cvpr20115995496  grefenstette edward blunsom phil de freitas nando hermann karl moritz 20140429 a deep architecture for semantic parsing arxiv14047296  cscl  learning semantic representations using convolutional neural networks for web search – microsoft research researchmicrosoftcom retrieved 20151217  kalchbrenner nal grefenstette edward blunsom phil 20140408 a convolutional neural network for modelling sentences arxiv14042188  cscl  kim yoon 20140825 convolutional neural networks for sentence classification arxiv14085882  cscl  collobert ronan and jason weston a unified architecture for natural language processing deep neural networks with multitask learningproceedings of the 25th international conference on machine learning acm 2008 collobert ronan weston jason bottou leon karlen michael kavukcuoglu koray kuksa pavel 20110302 natural language processing almost from scratch arxiv11030398  cslg  wallach izhar dzamba michael heifets abraham 20151009 atomnet a deep convolutional neural network for bioactivity prediction in structurebased drug discovery arxiv151002855  cslg  yosinski jason clune jeff nguyen anh fuchs thomas lipson hod 20150622 understanding neural networks through deep visualization arxiv150606579  cscv  toronto startup has a faster way to discover effective medicines the globe and mail retrieved 20151109  startup harnesses supercomputers to seek cures kqed future of you retrieved 20151109  chellapilla k fogel db 1999 evolving neural networks to play checkers without relying on expert knowledge ieee trans neural netw 10 6 1382–91 pmid 18252639 doi10110972809083  httpieeexploreieeeorgdocument942536 fogel david 2001 blondie24 playing at the edge of ai san francisco ca morgan kaufmann asin 1558607838 isbn 1558607838 cs1 maint asin uses isbn link clark christopher storkey amos 2014 teaching deep convolutional neural networks to play go arxiv14123409  csai  maddison chris j huang aja sutskever ilya silver david 2014 move evaluation in go using deep convolutional neural networks arxiv14126564  cslg  alphago – google deepmind retrieved 30 january 2016  durjoy sen maitra ujjwal bhattacharya sk parui cnn based common approach to handwritten character recognition of multiple scripts in document analysis and recognition icdar 2015 13th international conference on vol no pp1021–1025 23–26 aug 2015 mnih volodymyr et al 2015 humanlevel control through deep reinforcement learning nature 518 7540 529–533 bibcode2015natur518529m pmid 25719670 doi101038nature14236  sun r sessions c june 2000 selfsegmentation of sequences automatic formation of hierarchies of sequential behaviors ieee transactions on systems man and cybernetics part b cybernetics 30 3 403–418 issn 10834419 doi1011093477846230  convolutional deep belief networks on cifar10 pdf  lee honglak grosse roger ranganath rajesh ng andrew y 1 january 2009 convolutional deep belief networks for scalable unsupervised learning of hierarchical representations proceedings of the 26th annual international conference on machine learning – icml 09 acm 609–616 isbn 9781605585161 doi10114515533741553453 – via acm digital library  cade metz may 18 2016 google built its very own chips to power its ai bots wired  keras documentation kerasio  richards douglas e 20170430 infinity born paragon press isbn 1546406395  external linksedit convnetbenchmarks — easy benchmarking of all public opensource implementations of convnets with results beginners guide to convolutional neural nets — a gentle tutorial on how convolutional nets work cs231n convolutional neural networks for visual recognition — andrej karpathys stanford cs class an intuitive explanation of convolutional neural networks — a beginner level introduction to what convolutional neural networks are and how they work convolutional neural networks for image classification — literature survey retrieved from httpsenwikipediaorgwindexphptitleconvolutionalneuralnetworkoldid808605102 categories artificial neural networks computer vision computational neuroscience hidden categories cs1 maint asin uses isbn wikipedia articles needing clarification from october 2017 wikipedia articles needing clarification from july 2017 all articles with unsourced statements articles with unsourced statements from october 2017 wikipedia articles needing clarification from september 2016 articles needing additional references from june 2017 all articles needing additional references navigation menu personal tools not logged in talk contributions create account log in namespaces article talk variants views read edit view history more search navigation main page contents featured content current events random article donate to wikipedia wikipedia store interaction help about wikipedia community portal recent changes contact page tools what links here related changes upload file special pages permanent link page information wikidata item cite this page printexport create a book download as pdf printable version languages العربية català deutsch español فارسی français italiano 日本語 português русский українська 中文 edit links this page was last edited on 3 november 2017 at 2235 text is available under the creative commons attributionsharealike license additional terms may apply by using this site you agree to the terms of use and privacy policy wikipedia® is a registered trademark of the wikimedia foundation inc a nonprofit organization privacy policy about wikipedia disclaimers contact wikipedia developers cookie statement mobile view 
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/CNN_(disambiguation)
https://en.wikipedia.org/wiki/Machine_learning
https://en.wikipedia.org/wiki/Data_mining
https://en.wikipedia.org/wiki/File:Kernel_Machine.svg
https://en.wikipedia.org/wiki/Statistical_classification
https://en.wikipedia.org/wiki/Cluster_analysis
https://en.wikipedia.org/wiki/Regression_analysis
https://en.wikipedia.org/wiki/Anomaly_detection
https://en.wikipedia.org/wiki/Association_rule_learning
https://en.wikipedia.org/wiki/Reinforcement_learning
https://en.wikipedia.org/wiki/Structured_prediction
https://en.wikipedia.org/wiki/Feature_engineering
https://en.wikipedia.org/wiki/Feature_learning
https://en.wikipedia.org/wiki/Online_machine_learning
https://en.wikipedia.org/wiki/Semi-supervised_learning
https://en.wikipedia.org/wiki/Unsupervised_learning
https://en.wikipedia.org/wiki/Learning_to_rank
https://en.wikipedia.org/wiki/Grammar_induction
https://en.wikipedia.org/wiki/Supervised_learning
https://en.wikipedia.org/wiki/Statistical_classification
https://en.wikipedia.org/wiki/Regression_analysis
https://en.wikipedia.org/wiki/Decision_tree_learning
https://en.wikipedia.org/wiki/Ensemble_learning
https://en.wikipedia.org/wiki/Bootstrap_aggregating
https://en.wikipedia.org/wiki/Boosting_(machine_learning)
https://en.wikipedia.org/wiki/Random_forest
https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
https://en.wikipedia.org/wiki/Linear_regression
https://en.wikipedia.org/wiki/Naive_Bayes_classifier
https://en.wikipedia.org/wiki/Artificial_neural_network
https://en.wikipedia.org/wiki/Logistic_regression
https://en.wikipedia.org/wiki/Perceptron
https://en.wikipedia.org/wiki/Relevance_vector_machine
https://en.wikipedia.org/wiki/Support_vector_machine
https://en.wikipedia.org/wiki/Cluster_analysis
https://en.wikipedia.org/wiki/BIRCH
https://en.wikipedia.org/wiki/Hierarchical_clustering
https://en.wikipedia.org/wiki/K-means_clustering
https://en.wikipedia.org/wiki/Expectation–maximization_algorithm
https://en.wikipedia.org/wiki/DBSCAN
https://en.wikipedia.org/wiki/OPTICS_algorithm
https://en.wikipedia.org/wiki/Mean-shift
https://en.wikipedia.org/wiki/Dimensionality_reduction
https://en.wikipedia.org/wiki/Factor_analysis
https://en.wikipedia.org/wiki/Canonical_correlation_analysis
https://en.wikipedia.org/wiki/Independent_component_analysis
https://en.wikipedia.org/wiki/Linear_discriminant_analysis
https://en.wikipedia.org/wiki/Non-negative_matrix_factorization
https://en.wikipedia.org/wiki/Principal_component_analysis
https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
https://en.wikipedia.org/wiki/Structured_prediction
https://en.wikipedia.org/wiki/Graphical_model
https://en.wikipedia.org/wiki/Bayesian_network
https://en.wikipedia.org/wiki/Conditional_random_field
https://en.wikipedia.org/wiki/Hidden_Markov_model
https://en.wikipedia.org/wiki/Anomaly_detection
https://en.wikipedia.org/wiki/K-nearest_neighbors_classification
https://en.wikipedia.org/wiki/Local_outlier_factor
https://en.wikipedia.org/wiki/Artificial_neural_network
https://en.wikipedia.org/wiki/Autoencoder
https://en.wikipedia.org/wiki/Deep_learning
https://en.wikipedia.org/wiki/Multilayer_perceptron
https://en.wikipedia.org/wiki/Recurrent_neural_network
https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine
https://en.wikipedia.org/wiki/Self-organizing_map
https://en.wikipedia.org/wiki/Reinforcement_learning
https://en.wikipedia.org/wiki/Q-learning
https://en.wikipedia.org/wiki/State–action–reward–state–action
https://en.wikipedia.org/wiki/Temporal_difference_learning
https://en.wikipedia.org/wiki/Bias-variance_dilemma
https://en.wikipedia.org/wiki/Computational_learning_theory
https://en.wikipedia.org/wiki/Empirical_risk_minimization
https://en.wikipedia.org/wiki/Occam_learning
https://en.wikipedia.org/wiki/Probably_approximately_correct_learning
https://en.wikipedia.org/wiki/Statistical_learning_theory
https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory
https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems
https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning
https://en.wikipedia.org/wiki/Machine_Learning_(journal)
https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research
http://arxiv.org/list/cs.LG/recent
https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research
https://en.wikipedia.org/wiki/Outline_of_machine_learning
https://en.wikipedia.org/wiki/File:Portal-puzzle.svg
https://en.wikipedia.org/wiki/Portal:Machine_learning
https://en.wikipedia.org/wiki/Template:Machine_learning_bar
https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar
https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit
https://en.wikipedia.org/wiki/Machine_learning
https://en.wikipedia.org/wiki/Feedforward_neural_network
https://en.wikipedia.org/wiki/Artificial_neural_network
https://en.wikipedia.org/wiki/Multilayer_perceptron
https://en.wikipedia.org/wiki/Data_pre-processing
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Translation_invariance
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Mathematical_biology
https://en.wikipedia.org/wiki/Biological
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Artificial_neuron
https://en.wikipedia.org/wiki/Visual_cortex
https://en.wikipedia.org/wiki/Cortical_neuron
https://en.wikipedia.org/wiki/Visual_field
https://en.wikipedia.org/wiki/Receptive_field
https://en.wikipedia.org/w/index.php?title=Image_classification_algorithm&action=edit&redlink=1
https://en.wikipedia.org/w/index.php?title=Filter_(image_processing)&action=edit&redlink=1
https://en.wikipedia.org/wiki/Computer_vision
https://en.wikipedia.org/wiki/Recommender_system
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Natural_language_processing
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=1
https://en.wikipedia.org/w/index.php?title=Hidden_layer_(neural_network)&action=edit&redlink=1
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=2
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Translation_(geometry)
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Multilayer_perceptron
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Backpropagation
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=3
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=4
https://en.wikipedia.org/wiki/Multilayer_perceptron
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=5
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=6
https://en.wikipedia.org/wiki/Time_delay_neural_network
https://en.wikipedia.org/wiki/Image_classification
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=7
https://en.wikipedia.org/wiki/Living_organisms
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=8
https://en.wikipedia.org/wiki/David_H._Hubel
https://en.wikipedia.org/wiki/Torsten_Wiesel
https://en.wikipedia.org/wiki/Cortex_(anatomy)
https://en.wikipedia.org/wiki/Visual_field
https://en.wikipedia.org/wiki/Receptive_field
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Visual_field
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Simple_cells_(visual_cortex)
https://en.wikipedia.org/wiki/Complex_cells_(visual_cortex)
https://en.wikipedia.org/wiki/Receptive_field
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=9
https://en.wikipedia.org/wiki/Neocognitron
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Neocognitron
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=10
https://en.wikipedia.org/wiki/Yann_LeCun
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=11
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Electromyography
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=12
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=13
https://en.wikipedia.org/wiki/GPGPU
https://en.wikipedia.org/wiki/Machine_learning
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/GPU
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Database
https://en.wikipedia.org/wiki/MNIST_database
https://en.wikipedia.org/wiki/RGB_images
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=14
https://en.wikipedia.org/wiki/Multilayer_perceptron
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Curse_of_dimensionality
https://en.wikipedia.org/wiki/File:Conv_layers.png
https://en.wikipedia.org/wiki/File:Conv_layers.png
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Wikipedia:Please_clarify
https://en.wikipedia.org/wiki/Wikipedia:Citation_needed
https://en.wikipedia.org/wiki/Receptive_field
https://en.wikipedia.org/wiki/Translational_symmetry
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=15
https://en.wikipedia.org/wiki/File:Question_book-new.svg
https://en.wikipedia.org/wiki/Wikipedia:Verifiability
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit
https://en.wikipedia.org/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1
https://en.wikipedia.org/wiki/Help:Maintenance_template_removal
https://en.wikipedia.org/wiki/File:Conv_layer.png
https://en.wikipedia.org/wiki/File:Conv_layer.png
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=16
https://en.wikipedia.org/wiki/Kernel_(image_processing)
https://en.wikipedia.org/wiki/Dot_product
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=17
https://en.wikipedia.org/wiki/Hyperparameter_optimization
https://en.wikipedia.org/wiki/Receptive_field
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=18
https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Integer
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=19
https://en.wikipedia.org/wiki/Convolution
https://en.wikipedia.org/wiki/Kernel_(image_processing)
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=20
https://en.wikipedia.org/wiki/File:Max_pooling.png
https://en.wikipedia.org/wiki/File:Max_pooling.png
https://en.wikipedia.org/wiki/Overfitting
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/File:RoI_pooling_animated.gif
https://en.wikipedia.org/wiki/File:RoI_pooling_animated.gif
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Object_detection
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=21
https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
https://en.wikipedia.org/wiki/Activation_function
https://en.wikipedia.org/wiki/Hyperbolic_tangent
https://en.wikipedia.org/wiki/Sigmoid_function
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=22
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=23
https://en.wikipedia.org/wiki/Softmax_function
https://en.wikipedia.org/wiki/Sigmoid_function
https://en.wikipedia.org/wiki/Cross_entropy
https://en.wikipedia.org/wiki/Euclidean_distance
https://en.wikipedia.org/wiki/File:Typical_cnn.png
https://en.wikipedia.org/wiki/File:Typical_cnn.png
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=24
https://en.wikipedia.org/wiki/File:Question_book-new.svg
https://en.wikipedia.org/wiki/Wikipedia:Verifiability
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit
https://en.wikipedia.org/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1
https://en.wikipedia.org/wiki/Help:Maintenance_template_removal
https://en.wikipedia.org/wiki/Hyperparameter_optimization
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=25
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=26
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=27
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=28
https://en.wikipedia.org/wiki/Regularization
https://en.wikipedia.org/wiki/File:Question_book-new.svg
https://en.wikipedia.org/wiki/Wikipedia:Verifiability
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit
https://en.wikipedia.org/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1
https://en.wikipedia.org/wiki/Help:Maintenance_template_removal
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=29
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=30
https://en.wikipedia.org/wiki/Overfitting
https://en.wikipedia.org/wiki/Dropout_(neural_networks)
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=31
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=32
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Deterministic_algorithm
https://en.wikipedia.org/wiki/Multinomial_distribution
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=33
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=34
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=35
https://en.wikipedia.org/wiki/Early_stopping
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=36
https://en.wikipedia.org/wiki/Zero_norm
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=37
https://en.wikipedia.org/wiki/L1-norm
https://en.wikipedia.org/wiki/L2_norm
https://en.wikipedia.org/wiki/Elastic_net_regularization
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=38
https://en.wikipedia.org/wiki/Sparse_approximation
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=39
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Retina
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Visual_system
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=40
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=41
https://en.wikipedia.org/wiki/Per-comparison_error_rate
https://en.wikipedia.org/wiki/MNIST_database
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Facial_recognition_system
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Video_quality
https://en.wikipedia.org/wiki/Root_mean_square_error
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ImageNet_Large_Scale_Visual_Recognition_Challenge
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/DeepDream
https://en.wikipedia.org/wiki/Precision_and_recall
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=42
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=43
https://en.wikipedia.org/wiki/Natural_language_processing
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=44
https://en.wikipedia.org/wiki/Drug_discovery
https://en.wikipedia.org/wiki/Proteins
https://en.wikipedia.org/wiki/Drug_design
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Aromaticity
https://en.wikipedia.org/wiki/Orbital_hybridisation
https://en.wikipedia.org/wiki/Hydrogen_bond
https://en.wikipedia.org/wiki/Biomolecule
https://en.wikipedia.org/wiki/Ebola_virus
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Multiple_sclerosis
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=45
https://en.wikipedia.org/wiki/Draughts
https://en.wikipedia.org/wiki/David_B._Fogel
https://en.wikipedia.org/wiki/Blondie24
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Chinook_(draughts_player)
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=46
https://en.wikipedia.org/wiki/Computer_Go
https://en.wikipedia.org/wiki/GNU_Go
https://en.wikipedia.org/wiki/Monte_Carlo_tree_search
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Go_ranks_and_ratings
https://en.wikipedia.org/wiki/GNU_Go
https://en.wikipedia.org/wiki/Monte_Carlo_tree_search
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/AlphaGo
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=47
https://en.wikipedia.org/wiki/Overfitting
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=48
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=49
https://en.wikipedia.org/wiki/Q-learning
https://en.wikipedia.org/wiki/Reinforcement_learning
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Atari_2600
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=50
https://en.wikipedia.org/wiki/Deep_belief_network
https://en.wikipedia.org/wiki/Deep_belief_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=51
https://en.wikipedia.org/wiki/Caffe_(software)
https://en.wikipedia.org/wiki/C++
https://en.wikipedia.org/wiki/Python_(programming_language)
https://en.wikipedia.org/wiki/MATLAB
https://en.wikipedia.org/wiki/Deeplearning4j
https://en.wikipedia.org/wiki/Java_(programming_language)
https://en.wikipedia.org/wiki/Scala_(programming_language)
https://en.wikipedia.org/wiki/Apache_Spark
http://deeplearning4j.org/
https://github.com/deeplearning4j/libnd4j
http://hackage.haskell.org/package/deeplearning-hs
https://en.wikipedia.org/wiki/Haskell_(programming_language)
http://www.vlfeat.org/matconvnet/
https://en.wikipedia.org/wiki/MATLAB
http://mxnet.io/
https://github.com/NervanaSystems/neon
https://github.com/soumith/convnet-benchmarks/
https://en.wikipedia.org/wiki/TensorFlow
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Theano_(software)
https://en.wikipedia.org/wiki/NumPy
https://en.wikipedia.org/wiki/Torch_(machine_learning)
http://www.torch.ch/
https://en.wikipedia.org/wiki/C_(programming_language)
https://en.wikipedia.org/wiki/Lua_(programming_language)
https://en.wikipedia.org/wiki/Microsoft_Cognitive_Toolkit
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=52
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=53
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=54
https://en.wikipedia.org/wiki/Convolution
https://en.wikipedia.org/wiki/Deep_learning
https://en.wikipedia.org/wiki/Neocognitron
https://en.wikipedia.org/wiki/Scale-invariant_feature_transform
https://en.wikipedia.org/wiki/Time_delay_neural_network
https://en.wikipedia.org/wiki/Vision_processing_unit
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=55
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://yann.lecun.com/exdb/lenet/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://drive.google.com/file/d/0B65v6Wo67Tk5Zm03Tm1kaEdIYkE/view?usp=sharing
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://drive.google.com/file/d/0B65v6Wo67Tk5ODRzZmhSR29VeDg/view?usp=sharing
https://en.wikipedia.org/wiki/Bibcode
http://adsabs.harvard.edu/abs/1990ApOpt..29.4790Z
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/20577468
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1364/AO.29.004790
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/sparse/matsugo_etal_face_expression_conv_nnet.pdf
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1016/S0893-6080(03)00115-1
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://doi.acm.org/10.1145/1390156.1390177
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/978-1-60558-205-4
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1145/1390156.1390177
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://deeplearning.net/tutorial/lenet.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.worldcat.org/oclc/987790957
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/9783319575490
https://en.wikipedia.org/wiki/OCLC
https://www.worldcat.org/oclc/987790957
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.idsia.ch/~juergen/ijcai2011.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6248110
https://en.wikipedia.org/wiki/IEEE_Conference_on_Computer_Vision_and_Pattern_Recognition
https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/978-1-4673-1226-4
https://en.wikipedia.org/wiki/OCLC
https://www.worldcat.org/oclc/812295155
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1202.2745v1
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/CVPR.2012.6248110
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://hal.univ-nantes.fr/docs/00/28/74/26/PDF/A_convolutional_neural_network_approach_for_objective_video_quality_assessment_completefinal_manuscript.pdf
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/17001990
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/TNN.2006.879766
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912
https://en.wikipedia.org/wiki/International_Standard_Serial_Number
https://www.worldcat.org/issn/0022-3751
https://en.wikipedia.org/wiki/PubMed_Central
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/4966457
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1113/jphysiol.1968.sp008455
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Bibcode
http://adsabs.harvard.edu/abs/2015Natur.521..436L
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/26017442
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1038/nature14539
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/7370364
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1007/BF00344251
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://psych.stanford.edu/~jlm/papers/PDP/Volume%201/Chap8_PDP86.pdf
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/9780262680530
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://papers.nips.cc/paper/20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/5.726791
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://drive.google.com/file/d/0B65v6Wo67Tk5dkJTcEMtU2c5Znc/view?usp=sharing
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://drive.google.com/file/d/0B65v6Wo67Tk5cm5DTlNGd0NPUmM/view?usp=sharing
https://en.wikipedia.org/wiki/Bibcode
http://adsabs.harvard.edu/abs/1991ApOpt..30.4211Z
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/20706526
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1364/AO.30.004211
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://drive.google.com/file/d/0B65v6Wo67Tk5Ml9qeW5nQ3poVTQ/view?usp=sharing
https://en.wikipedia.org/wiki/Bibcode
http://adsabs.harvard.edu/abs/1994MedPh..21..517Z
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/8058017
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1118/1.597177
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.ais.uni-bonn.de/books/LNCS2766.pdf
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-40722-5
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1007/b11963
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://hal.inria.fr/inria-00112631/document
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/16764513
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1162/neco.2006.18.7.1527
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://yann.lecun.com/exdb/publis/pdf/ranzato-06.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://cs231n.github.io/convolutional-networks/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1412.6071
https://arxiv.org/archive/cs.CV
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1412.6806
https://arxiv.org/archive/cs.LG
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://deepsense.io/region-of-interest-pooling-explained/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1504.08083
https://arxiv.org/archive/cs.CV
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.deeplearningpatterns.com
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://jmlr.org/proceedings/papers/v28/wan13.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1301.3557
https://arxiv.org/archive/cs.LG
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://research.microsoft.com/apps/pubs/?id=68920
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1207.0580
https://arxiv.org/archive/cs.NE
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://jmlr.org/papers/v15/srivastava14a.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1016/s0364-0213(79)80008-7
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.coursera.org/learn/neural-networks
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/CiteSeerX
https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.5813
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/72.554195
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.image-net.org/challenges/LSVRC/2014/results
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1409.4842
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1409.0575
https://arxiv.org/archive/cs.CV
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.technologyreview.com/view/535201/the-face-detection-algorithm-set-to-revolutionize-image-search
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://link.springer.com/chapter/10.1007/978-3-642-25446-8_4
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-25445-1
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1007/978-3-642-25446-8_4
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165309
https://en.wikipedia.org/wiki/International_Standard_Serial_Number
https://www.worldcat.org/issn/0162-8828
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/22392705
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/TPAMI.2012.59
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1406.2199
https://arxiv.org/archive/cs.CV
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://dl.acm.org/citation.cfm?id=1888212.1888225
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/3-642-15566-9
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/978-1-4577-0394-2
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/CVPR.2011.5995496
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1404.7296
https://arxiv.org/archive/cs.CL
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://research.microsoft.com/apps/pubs/default.aspx?id=214617
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1404.2188
https://arxiv.org/archive/cs.CL
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1408.5882
https://arxiv.org/archive/cs.CL
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1103.0398
https://arxiv.org/archive/cs.LG
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1510.02855
https://arxiv.org/archive/cs.LG
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1506.06579
https://arxiv.org/archive/cs.CV
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.theglobeandmail.com/report-on-business/small-business/starting-out/toronto-startup-has-a-faster-way-to-discover-effective-medicines/article25660419/
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ww2.kqed.org/futureofyou/2015/05/27/startup-harnesses-supercomputers-to-seek-cures/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/18252639
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/72.809083
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/document/942536/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Www.davidfogel.com&action=edit&redlink=1
https://en.wikipedia.org/wiki/Amazon_Standard_Identification_Number
https://www.amazon.com/dp/1558607838
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/1558607838
https://en.wikipedia.org/wiki/Category:CS1_maint:_ASIN_uses_ISBN
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1412.3409
https://arxiv.org/archive/cs.AI
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/ArXiv
https://arxiv.org/abs/1412.6564
https://arxiv.org/archive/cs.LG
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.deepmind.com/alpha-go.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7333916&tag=1
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Bibcode
http://adsabs.harvard.edu/abs/2015Natur.518..529M
https://en.wikipedia.org/wiki/PubMed_Identifier
https://www.ncbi.nlm.nih.gov/pubmed/25719670
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1038/nature14236
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/document/846230/
https://en.wikipedia.org/wiki/International_Standard_Serial_Number
https://www.worldcat.org/issn/1083-4419
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1109/3477.846230
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://doi.acm.org/10.1145/1553374.1553453
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/9781605585161
https://en.wikipedia.org/wiki/Digital_object_identifier
https://doi.org/10.1145/1553374.1553453
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.wired.com/2016/05/google-tpu-custom-chips/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://keras.io/
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.amazon.com/Infinity-Born-Douglas-E-Richards-ebook/dp/B072584M83
https://en.wikipedia.org/wiki/International_Standard_Book_Number
https://en.wikipedia.org/wiki/Special:BookSources/1546406395
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit&section=56
https://github.com/soumith/convnet-benchmarks
http://deeplearning4j.org/convolutionalnets.html
https://cs231n.github.io/
https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
https://www.completegate.com/2017022864/blog/deep-machine-learning-images-lenet-alexnet-cnn/all-pages
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=808605102
https://en.wikipedia.org/wiki/Help:Category
https://en.wikipedia.org/wiki/Category:Artificial_neural_networks
https://en.wikipedia.org/wiki/Category:Computer_vision
https://en.wikipedia.org/wiki/Category:Computational_neuroscience
https://en.wikipedia.org/wiki/Category:CS1_maint:_ASIN_uses_ISBN
https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_October_2017
https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2017
https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements
https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_October_2017
https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2016
https://en.wikipedia.org/wiki/Category:Articles_needing_additional_references_from_June_2017
https://en.wikipedia.org/wiki/Category:All_articles_needing_additional_references
https://en.wikipedia.org/wiki/Special:MyTalk
https://en.wikipedia.org/wiki/Special:MyContributions
https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Convolutional+neural+network
https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Convolutional+neural+network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/wiki/Talk:Convolutional_neural_network
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=edit
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=history
https://en.wikipedia.org/wiki/Main_Page
https://en.wikipedia.org/wiki/Main_Page
https://en.wikipedia.org/wiki/Portal:Contents
https://en.wikipedia.org/wiki/Portal:Featured_content
https://en.wikipedia.org/wiki/Portal:Current_events
https://en.wikipedia.org/wiki/Special:Random
https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en
https://shop.wikimedia.org
https://en.wikipedia.org/wiki/Help:Contents
https://en.wikipedia.org/wiki/Wikipedia:About
https://en.wikipedia.org/wiki/Wikipedia:Community_portal
https://en.wikipedia.org/wiki/Special:RecentChanges
https://en.wikipedia.org/wiki/Wikipedia:Contact_us
https://en.wikipedia.org/wiki/Special:WhatLinksHere/Convolutional_neural_network
https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Convolutional_neural_network
https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard
https://en.wikipedia.org/wiki/Special:SpecialPages
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=808605102
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&action=info
https://www.wikidata.org/wiki/Special:EntityPage/Q17084460
https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Convolutional_neural_network&id=808605102
https://en.wikipedia.org/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Convolutional+neural+network
https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&page=Convolutional+neural+network&action=show-download-screen
https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&printable=yes
https://ar.wikipedia.org/wiki/شبكة_عصبونية_التفافية
https://ca.wikipedia.org/wiki/Xarxa_neuronal_convolucional
https://de.wikipedia.org/wiki/Convolutional_Neural_Network
https://es.wikipedia.org/wiki/Redes_neuronales_convolucionales
https://fa.wikipedia.org/wiki/شبکه_عصبی_پیچشی
https://fr.wikipedia.org/wiki/Réseau_neuronal_convolutif
https://it.wikipedia.org/wiki/Rete_neurale_convoluzionale
https://ja.wikipedia.org/wiki/畳み込みニューラルネットワーク
https://pt.wikipedia.org/wiki/Rede_neural_convolucional
https://ru.wikipedia.org/wiki/Свёрточная_нейронная_сеть
https://uk.wikipedia.org/wiki/Згорткова_нейронна_мережа
https://zh.wikipedia.org/wiki/卷积神经网络
https://www.wikidata.org/wiki/Special:EntityPage/Q17084460
https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License
https://creativecommons.org/licenses/by-sa/3.0/
https://wikimediafoundation.org/wiki/Terms_of_Use
https://wikimediafoundation.org/wiki/Privacy_policy
https://www.wikimediafoundation.org/
https://wikimediafoundation.org/wiki/Privacy_policy
https://en.wikipedia.org/wiki/Wikipedia:About
https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer
https://en.wikipedia.org/wiki/Wikipedia:Contact_us
https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute
https://wikimediafoundation.org/wiki/Cookie_statement
https://en.m.wikipedia.org/w/index.php?title=Convolutional_neural_network&mobileaction=toggle_view_mobile
https://wikimediafoundation.org/
https://www.mediawiki.org/
